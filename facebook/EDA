df = spark.read.csv("new.csv", inferSchema = True, header = True, quote = "\"", escape = "\"")

import pyspark.sql.functions as F
df.select('targetedness').withColumn('isNull_targetedness',F.col('targetedness').isNull()).where('isNull_targetedness = True').count()
