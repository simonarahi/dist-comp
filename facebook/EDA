df = spark.read.csv("/spring2020/data/fbpac-ads-en-US.csv", inferSchema = True, header = True, quote = "\"", escape = "\"")

# getting number of nulls for one specific column
import pyspark.sql.functions as F
df.select('targetedness').withColumn('isNull_targetedness',F.col('targetedness').isNull()).where('isNull_targetedness = True').count()

#getting NaN and Null values for each column
df_orders.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_orders.columns]).show()

# changing types from string to relevant type
from pyspark.sql.types import *
df = df.withColumn("political1", df["targetedness"].cast(FloatType())).drop("targetedness").withColumnRenamed("political1", "targetedness")
 
# getting dates in right format
def clean(row):
 if type(row[5])!=type(None):
  cleaned = row[5][:10]
  x = (cleaned, 1)
  return x
 
d= rdd.map(clean)

def sum(x,y):
  return x+y
  
# filter the None fields 
d1=d.filter(lambda x: x is not None)

# get counts by date
d2=d1.reduceByKey(sum)
# get which date had the most ads created at
d2.max(lambda x:x[1])
[out]: ('2018-11-06', 1740)
# sorting 
sorts = d2.sortBy(lambda x:x[1], ascending=False)

def cleaner(row):
   for field in range(len(row)):
     if type(row[field])!=type(None):
        return True
     else:
        return False

(['id', 'political', 'not_political', 'title', 'message', 'created_at', 'updated_at', 'lang', 'impressions', 'political_probability', 'suppressed', 'targets', 'advertiser', 'entities', 'paid_for_by', 'targetedness'])

# EDA on sentiment analysis
def counter(row):
 x=row[8]
 return x
 
 # Returning RDD with emotional vector and political probability
def mapper(row):
  if type(row[9])== float:
  y=(row[3],row[9])
  return y
 
 pol = newdata.map(mapper)
 
 # filter ads less than political prob of 0.95 (mean is 0.93)
def political(row):
  if row[1]>0.93:
   return True
  else:
   return False
 
 # writing dataframe to single csv
 df.coalesce(1).write.option("header", "true").csv("sample_file.csv")
 
